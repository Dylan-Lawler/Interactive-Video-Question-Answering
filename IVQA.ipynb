{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries and Mount Google Drive:"
      ],
      "metadata": {
        "id": "uuwEYhE5Mj3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "tXTT39DD2BRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "MTA3hnvc4YQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/MSVD-QA/'\n",
        "video_path = data_path + 'video/'\n",
        "train_qa_path = data_path + 'train_qa.json'\n",
        "clarify_train_qa_path = data_path + 'clarify_train_qa.json'\n",
        "val_qa_path = data_path + 'val_qa.json'\n",
        "\n",
        "# Function to load JSON data\n",
        "def load_json_data(path):\n",
        "    with open(path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Loading QA data\n",
        "train_qa = load_json_data(train_qa_path)\n",
        "clarify_train_qa = load_json_data(clarify_train_qa_path)\n",
        "val_qa = load_json_data(val_qa_path)\n",
        "\n",
        "# Convert QA data to DataFrame\n",
        "train_df = pd.DataFrame(train_qa)\n",
        "clarify_train_df = pd.DataFrame(clarify_train_qa)\n",
        "val_df = pd.DataFrame(val_qa)\n",
        "\n",
        "name_mapping_path = data_path + 'youtube_mapping.txt'\n",
        "\n",
        "def load_name_mapping(path):\n",
        "    mapping = {}\n",
        "    with open(path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                video_name, video_id = parts\n",
        "                mapping[video_id] = video_name\n",
        "    return mapping\n",
        "\n",
        "name_mapping = load_name_mapping(name_mapping_path)\n"
      ],
      "metadata": {
        "id": "6lNIBrbQ4T9C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Video Frames and Extract Features"
      ],
      "metadata": {
        "id": "2MAQ_oxqzdzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load video frames using name mapping\n",
        "def load_video_frames(video_id, max_frames=120):\n",
        "    video_name = name_mapping.get('vid' + str(video_id), None)\n",
        "    if not video_name:\n",
        "        raise ValueError(f\"No video found for video_id: {video_id}\")\n",
        "\n",
        "    video_full_path = video_path + video_name + '.avi'\n",
        "    cap = cv2.VideoCapture(video_full_path)\n",
        "    if not cap.isOpened():\n",
        "        raise IOError(f\"Cannot open video file: {video_full_path}\")\n",
        "\n",
        "    frames = []\n",
        "    try:\n",
        "        while len(frames) < max_frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.resize(frame, (224, 224))  # Resize to model's input size\n",
        "            frame = frame.astype('float32') / 255.0  # Normalization\n",
        "            frames.append(frame)\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "# TensorFlow Hub model\n",
        "model_url = 'https://tfhub.dev/deepmind/i3d-kinetics-400/1'\n",
        "video_model = hub.KerasLayer(model_url)\n",
        "\n",
        "def extract_features(frames):\n",
        "    # Ensure frames are in the right shape: (batch_size, num_frames, height, width, channels)\n",
        "    frames = np.expand_dims(frames, axis=0)  # Add batch dimension\n",
        "    features = video_model(frames)  # Extract features\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "_ofPhhMu4udM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Bert Model"
      ],
      "metadata": {
        "id": "bvRGZvvpzkol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT for QA\n",
        "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "qa_model = TFBertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "metadata": {
        "id": "Wb9TTjCz48aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precompute and Cache Videos"
      ],
      "metadata": {
        "id": "jj0mfjDVhnq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Precompute and cache video features\n",
        "video_features_cache = {}\n",
        "\n",
        "for video_id in clarify_train_df['video_id'].unique():\n",
        "    print(video_id)\n",
        "    frames = load_video_frames(video_id)\n",
        "    video_features = extract_features(frames)\n",
        "    video_features_cache[video_id] = video_features\n"
      ],
      "metadata": {
        "id": "hr940O_fhnQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training\n"
      ],
      "metadata": {
        "id": "D_7iFw-QzoKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for _, row in clarify_train_df.iterrows():\n",
        "    video_id = row['video_id']\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    clarifications = row['clarifications']\n",
        "\n",
        "    # Retrieve precomputed video features\n",
        "    video_features = video_features_cache[video_id]\n",
        "    video_features = tf.reshape(video_features, [1, -1])  # Reshape to (1, num_features)\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    encoded_dict = qa_tokenizer.encode_plus(\n",
        "        question,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf',\n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    for clarification in clarifications:\n",
        "        clarifying_question = clarification['clarifying_question']\n",
        "        clarifying_answer = clarification['clarifying_answer']\n",
        "        clarifying_text = clarifying_question + \" \" + clarifying_answer\n",
        "\n",
        "        encoded_dict = qa_tokenizer.encode_plus(\n",
        "            clarifying_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = tf.concat(input_ids, axis=0)\n",
        "    attention_masks = tf.concat(attention_masks, axis=0)\n",
        "\n",
        "    text_features = qa_model([input_ids, attention_masks])[0]\n",
        "    text_features = tf.reduce_mean(text_features, axis=0)\n",
        "    text_features = tf.expand_dims(text_features, axis=0)\n",
        "\n",
        "    # Ensure video and text features have compatible dimensions\n",
        "    print(\"Video features shape:\", video_features.shape)  # Expected shape: (1, num_features)\n",
        "    print(\"Text features shape:\", text_features.shape)    # Expected shape: (1, 128)\n",
        "\n",
        "    combined_features = tf.concat([video_features, text_features], axis=1)\n",
        "\n",
        "    X_train.append(combined_features.numpy())\n",
        "    y_train.append(answer)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n"
      ],
      "metadata": {
        "id": "olUcmGSIzJzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define and Compile Model"
      ],
      "metadata": {
        "id": "VmCyW60Tzxgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple classifier\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu', input_shape=(combined_features.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(len(clarify_train_df['answer'].unique()), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "r2fjsZypzNQH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "kPdRZM9Vz4Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "YVKBgyJszUht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model"
      ],
      "metadata": {
        "id": "-Ugwsj2vqiQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the test data\n",
        "test_qa_path = '/content/drive/MyDrive/MSVD-QA/test_qa.json'\n",
        "\n",
        "def load_json_data(path):\n",
        "    with open(path, 'r') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "# Load the test QA data\n",
        "test_qa = load_json_data(test_qa_path)\n",
        "test_df = pd.DataFrame(test_qa)\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "qa_model = TFBertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Define a function to calculate confidence score\n",
        "def get_confidence_score(logits):\n",
        "    probs = tf.nn.softmax(logits, axis=-1)\n",
        "    confidence = tf.reduce_max(probs, axis=-1)\n",
        "    return confidence.numpy()\n",
        "\n",
        "# Define a function to generate clarifying question\n",
        "def generate_clarifying_question(question, context):\n",
        "    input_text = f\"Q: {question} Context: {context} What is unclear?\"\n",
        "    encoded_dict = qa_tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf',\n",
        "    )\n",
        "    input_ids = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask']\n",
        "\n",
        "    outputs = qa_model(input_ids, attention_mask=attention_mask)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    start_idx = tf.argmax(start_scores, axis=1).numpy()[0]\n",
        "    end_idx = tf.argmax(end_scores, axis=1).numpy()[0]\n",
        "    clarifying_question = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(input_ids[0][start_idx:end_idx+1]))\n",
        "    return clarifying_question\n",
        "\n",
        "# Define a function to test the model\n",
        "def test_ivqa_model(test_df, video_features_cache, model, threshold=0.8):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    confidence_scores = []\n",
        "\n",
        "    for index, row in test_df.iterrows():\n",
        "        video_id = row['video_id']\n",
        "        question = row['question']\n",
        "        true_answer = row['answer']\n",
        "\n",
        "        # Retrieve precomputed video features\n",
        "        video_features = video_features_cache[video_id]\n",
        "        video_features = tf.reshape(video_features, [1, -1])  # Reshape to (1, num_features)\n",
        "\n",
        "        # Encode the question\n",
        "        encoded_dict = qa_tokenizer.encode_plus(\n",
        "            question,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_dict['input_ids']\n",
        "        attention_mask = encoded_dict['attention_mask']\n",
        "\n",
        "        text_features = qa_model([input_ids, attention_mask])[0]\n",
        "        text_features = tf.reduce_mean(text_features, axis=0)\n",
        "        text_features = tf.expand_dims(text_features, axis=0)\n",
        "\n",
        "        combined_features = tf.concat([video_features, text_features], axis=1)\n",
        "\n",
        "        # Get model prediction and confidence score\n",
        "        logits = model(combined_features)\n",
        "        confidence_score = get_confidence_score(logits)\n",
        "\n",
        "        if confidence_score < threshold:\n",
        "            # Generate clarifying question\n",
        "            clarifying_question = generate_clarifying_question(question, \"\")\n",
        "            print(f\"Clarifying question: {clarifying_question}\")\n",
        "\n",
        "            # Get clarifying answer from the user\n",
        "            clarifying_answer = input(\"Please provide the clarifying answer: \")\n",
        "\n",
        "            # Encode the clarifying question and answer\n",
        "            clarifying_text = clarifying_question + \" \" + clarifying_answer\n",
        "            encoded_dict = qa_tokenizer.encode_plus(\n",
        "                clarifying_text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=128,\n",
        "                truncation=True,\n",
        "                pad_to_max_length=True,\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='tf',\n",
        "            )\n",
        "\n",
        "            input_ids = tf.concat([input_ids, encoded_dict['input_ids']], axis=0)\n",
        "            attention_mask = tf.concat([attention_mask, encoded_dict['attention_mask']], axis=0)\n",
        "\n",
        "            text_features = qa_model([input_ids, attention_mask])[0]\n",
        "            text_features = tf.reduce_mean(text_features, axis=0)\n",
        "            text_features = tf.expand_dims(text_features, axis=0)\n",
        "\n",
        "            combined_features = tf.concat([video_features, text_features], axis=1)\n",
        "\n",
        "        # Get final model prediction\n",
        "        logits = model(combined_features)\n",
        "        predicted_answer = tf.argmax(logits, axis=-1).numpy()[0]\n",
        "\n",
        "        y_true.append(true_answer)\n",
        "        y_pred.append(predicted_answer)\n",
        "        confidence_scores.append(confidence_score)\n",
        "\n",
        "    return y_true, y_pred, confidence_scores\n",
        "\n",
        "# Assuming `video_features_cache` and `model` are preloaded and defined\n",
        "y_true, y_pred, confidence_scores = test_ivqa_model(test_df, video_features_cache, model)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "NDrsexiw188S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}